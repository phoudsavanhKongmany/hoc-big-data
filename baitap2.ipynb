{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMoZouSrm+qUyOqJpBGY8oA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phoudsavanhKongmany/hoc-big-data/blob/main/baitap2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgBXMTCE2tL2",
        "outputId": "9c8d9bfe-35a9-4a14-f792-cbca1d978319"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-13 14:57:52--  https://www.gutenberg.org/cache/epub/1513/pg1513.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 169546 (166K) [text/plain]\n",
            "Saving to: ‚Äòromeo.txt‚Äô\n",
            "\n",
            "\rromeo.txt             0%[                    ]       0  --.-KB/s               \rromeo.txt           100%[===================>] 165.57K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2026-01-13 14:57:52 (5.06 MB/s) - ‚Äòromeo.txt‚Äô saved [169546/169546]\n",
            "\n",
            "‚úÖ ƒê√£ chu·∫©n b·ªã xong d·ªØ li·ªáu: big_romeo.txt\n",
            "üöÄ Spark ƒë√£ s·∫µn s√†ng!\n"
          ]
        }
      ],
      "source": [
        "#PH·∫¶N 1: KH·ªûI T·∫†O M√îI TR∆Ø·ªúNG (SETUP)\n",
        "\n",
        "# 1. C√†i ƒë·∫∑t PySpark (Ch·∫°y tr√™n Google Colab)\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip install -q pyspark\n",
        "\n",
        "# 2. T·∫£i d·ªØ li·ªáu m·∫´u (Romeo & Juliet)\n",
        "!wget -O romeo.txt https://www.gutenberg.org/cache/epub/1513/pg1513.txt\n",
        "\n",
        "# 3. Nh√¢n b·∫£n d·ªØ li·ªáu ƒë·ªÉ gi·∫£ l·∫≠p Big Data (x100 l·∫ßn)\n",
        "# File g·ªëc ch·ªâ v√†i trƒÉm KB, nh√¢n l√™n ƒë·ªÉ th·∫•y r√µ t·ªëc ƒë·ªô x·ª≠ l√Ω\n",
        "with open(\"romeo.txt\", \"r\") as f:content = f.read()\n",
        "with open(\"big_romeo.txt\", \"w\") as f:\n",
        "  for _ in range(100):f.write(content + \"\\n\")\n",
        "\n",
        "print(\"‚úÖ ƒê√£ chu·∫©n b·ªã xong d·ªØ li·ªáu: big_romeo.txt\")\n",
        "\n",
        "# 4. Kh·ªüi t·∫°o Spark Session\n",
        "from pyspark.sql import SparkSession\n",
        "import time\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        ".appName(\"EvolutionOfSpark\") \\\n",
        ".master(\"local[*]\") \\\n",
        ".getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext # L·∫•y SparkContext ƒë·ªÉ d√πng cho RDD c≈©\n",
        "print(\"üöÄ Spark ƒë√£ s·∫µn s√†ng!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PH·∫¶N 2: C√ÅCH 1 - GI·∫¢I B·∫∞NG RDD (T∆Ø DUY MAPREDUCE C≈®)\n",
        "\n",
        "print(\"‚è≥ ƒêang ch·∫°y RDD (C√°ch c≈©)...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# 1. ƒê·ªçc file v√†o RDD\n",
        "rdd_text = sc.textFile(\"big_romeo.txt\")\n",
        "\n",
        "# 2. Th·ª±c hi·ªán MapReduce\n",
        "# - flatMap: C·∫Øt d√≤ng th√†nh t·ª´\n",
        "# - map: G√°n s·ªë 1 cho m·ªói t·ª´ -> (word, 1)\n",
        "# - reduceByKey: C·ªông d·ªìn c√°c s·ªë 1 l·∫°i\n",
        "rdd_counts = rdd_text.flatMap(lambda line: line.split(\" \")) \\\n",
        ".map(lambda word: (word, 1)) \\\n",
        ".reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# 3. S·∫Øp x·∫øp (Sort) - Vi·ªác n√†y r·∫•t t·ªën k√©m v·ªõi RDD\n",
        "# Ph·∫£i ƒë·∫£o ng∆∞·ª£c (count, word) ƒë·ªÉ sort theo key l√† count\n",
        "rdd_sorted = rdd_counts.map(lambda x: (x[1], x[0])) \\\n",
        ".sortByKey(ascending=False)\n",
        "\n",
        "# 4. L·∫•y k·∫øt qu·∫£ (Action)\n",
        "result_rdd = rdd_sorted.take(10)\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"‚è±Ô∏è Th·ªùi gian ch·∫°y RDD: {end_time - start_time:.4f} gi√¢y\")\n",
        "print(\"Top 10 t·ª´ (RDD):\", result_rdd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlybnkot3U7T",
        "outputId": "51d45513-d68f-4a19-e1f3-8ed94d669efc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ ƒêang ch·∫°y RDD (C√°ch c≈©)...\n",
            "‚è±Ô∏è Th·ªùi gian ch·∫°y RDD: 10.6638 gi√¢y\n",
            "Top 10 t·ª´ (RDD): [(168900, ''), (78000, 'the'), (55100, 'I'), (54100, 'and'), (52400, 'to'), (47600, 'of'), (45800, 'a'), (35000, 'in'), (31300, 'is'), (30400, 'my')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PH·∫¶N 3: C√ÅCH 2 - GI·∫¢I B·∫∞NG DATAFRAME (T∆Ø DUY C·∫§U TR√öC)\n",
        "\n",
        "from pyspark.sql.functions import split, explode, col, desc\n",
        "\n",
        "print(\"‚è≥ ƒêang ch·∫°y DataFrame (C√°ch m·ªõi)...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# 1. ƒê·ªçc file v√†o DataFrame (C√≥ c·∫•u tr√∫c d√≤ng/c·ªôt)\n",
        "df_text = spark.read.text(\"big_romeo.txt\")\n",
        "\n",
        "# 2. X·ª≠ l√Ω\n",
        "# - split: T√°ch chu·ªói\n",
        "# - explode: Bi·∫øn m·∫£ng th√†nh c√°c d√≤ng ri√™ng bi·ªát\n",
        "df_counts = df_text.select(explode(split(col(\"value\"), \" \")).alias(\"word\")) \\\n",
        ".groupBy(\"word\") \\\n",
        ".count() \\\n",
        ".orderBy(desc(\"count\"))\n",
        "\n",
        "# 3. L·∫•y k·∫øt qu·∫£\n",
        "result_df = df_counts.limit(10).collect()\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"‚è±Ô∏è Th·ªùi gian ch·∫°y DataFrame: {end_time - start_time:.4f} gi√¢y\")\n",
        "print(\"------------------------------------------------\")\n",
        "df_counts.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CqcT2GP3fSL",
        "outputId": "07122034-1e8a-4201-f072-0a08fe4d6ae4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ ƒêang ch·∫°y DataFrame (C√°ch m·ªõi)...\n",
            "‚è±Ô∏è Th·ªùi gian ch·∫°y DataFrame: 8.6006 gi√¢y\n",
            "------------------------------------------------\n",
            "+----+------+\n",
            "|word| count|\n",
            "+----+------+\n",
            "|    |168900|\n",
            "| the| 78000|\n",
            "|   I| 55100|\n",
            "| and| 54100|\n",
            "|  to| 52400|\n",
            "|  of| 47600|\n",
            "|   a| 45800|\n",
            "|  in| 35000|\n",
            "|  is| 31300|\n",
            "|  my| 30400|\n",
            "+----+------+\n",
            "only showing top 10 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PH·∫¶N 4: C√ÅCH 3 - GI·∫¢I B·∫∞NG SPARK SQL (T∆Ø DUY QUEN THU·ªòC)\n",
        "\n",
        "print(\"‚è≥ ƒêang ch·∫°y Spark SQL...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# 1. Bi·∫øn DataFrame ban ƒë·∫ßu th√†nh b·∫£ng t·∫°m (Temp View)\n",
        "# Nh·ªõ l·∫°i b∆∞·ªõc tr∆∞·ªõc: df_text ch·ª©a 1 c·ªôt l√† \"value\"\n",
        "df_text.createOrReplaceTempView(\"raw_book\")\n",
        "\n",
        "# 2. Vi·∫øt SQL\n",
        "# Logic: Split xong Explode ngay trong c√¢u SQL (Spark h·ªó tr·ª£ r·∫•t m·∫°nh)\n",
        "query = \"\"\"\n",
        "SELECT word, count(*) as total\n",
        "FROM (\n",
        "SELECT explode(split(value, ' ')) as word\n",
        "FROM raw_book\n",
        ")\n",
        "GROUP BY word\n",
        "ORDER BY total DESC\n",
        "\"\"\"\n",
        "\n",
        "# 3. Th·ª±c thi\n",
        "df_sql = spark.sql(query)\n",
        "result_sql = df_sql.limit(10).collect()\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"‚è±Ô∏è Th·ªùi gian ch·∫°y SQL: {end_time - start_time:.4f} gi√¢y\")\n",
        "df_sql.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GG-5vwGD3k6z",
        "outputId": "6515e383-ca9d-4e2e-8607-55f2609ca217"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ ƒêang ch·∫°y Spark SQL...\n",
            "‚è±Ô∏è Th·ªùi gian ch·∫°y SQL: 2.0049 gi√¢y\n",
            "+----+------+\n",
            "|word| total|\n",
            "+----+------+\n",
            "|    |168900|\n",
            "| the| 78000|\n",
            "|   I| 55100|\n",
            "| and| 54100|\n",
            "|  to| 52400|\n",
            "|  of| 47600|\n",
            "|   a| 45800|\n",
            "|  in| 35000|\n",
            "|  is| 31300|\n",
            "|  my| 30400|\n",
            "+----+------+\n",
            "only showing top 10 rows\n"
          ]
        }
      ]
    }
  ]
}